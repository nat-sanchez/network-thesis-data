# -*- coding: utf-8 -*-
"""Clustering Capas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p_XtdmMl5qzspe-7AUyYsdp7p2SKLhEY
"""

# --- Requisitos ---
# pip install pandas openpyxl networkx numpy

import re
import math
import pathlib
import numpy as np
import pandas as pd
import networkx as nx
from networkx.algorithms import bipartite as bx

# === CONFIGURACIÓN ===
INPUT_DIR = "capas_excel"   # Carpeta con los .xlsx (uno por capa)
ANIOS_PERMITIDOS = set(range(2012, 2018))  # 2012–2017

# === UTILIDADES ===
def infer_year(text: str):
    """Extrae un año (4 dígitos) de un texto; devuelve int o None."""
    m = re.search(r"(19|20)\d{2}", text or "")
    return int(m.group(0)) if m else None

def layer_from_filename(path: pathlib.Path):
    """Nombre de capa a partir del nombre de archivo (sin extensión)."""
    return path.stem

def to_numeric_matrix(df: pd.DataFrame) -> np.ndarray:
    """
    Convierte el DataFrame leído de Excel en matriz numérica:
    - Elimina filas/columnas totalmente vacías.
    - Intenta convertir todo a numérico; lo no convertible -> 0.
    - Redondea a 0/1 si parece booleana.
    """
    df2 = df.copy()
    df2 = df2.dropna(how="all").dropna(axis=1, how="all")
    df_num = df2.apply(pd.to_numeric, errors="coerce")
    # Heurística para recortar posibles encabezados textuales:
    if df_num.shape[0] > 0 and df_num.iloc[0].isna().mean() > 0.5:
        df_num = df_num.iloc[1:, :]
    if df_num.shape[1] > 0 and df_num.iloc[:, 0].isna().mean() > 0.5:
        df_num = df_num.iloc[:, 1:]
    df_num = df_num.apply(pd.to_numeric, errors="coerce").fillna(0.0)
    M = df_num.to_numpy()
    M[M < 0] = 0
    if np.all((M % 1) == 0) and np.unique(M).size <= 3:
        M = (M > 0).astype(int)
    return M

def build_bipartite_from_matrix(M: np.ndarray):
    """
    Construye grafo bipartito desde:
    - Incidencia B (rectangular): filas=U, columnas=V
    - Adyacencia A (cuadrada): detecta bipartición
    Devuelve: G (nx.Graph), part_U (set), part_V (set)
    """
    r, c = M.shape
    if r == 0 or c == 0:
        raise ValueError("Matriz vacía.")
    if r != c:
        # Incidencia B
        G = nx.Graph()
        U = [f"u_{i}" for i in range(r)]
        V = [f"v_{j}" for j in range(c)]
        G.add_nodes_from(U, bipartite=0)
        G.add_nodes_from(V, bipartite=1)
        rows, cols = np.where(M > 0)
        G.add_edges_from((U[i], V[j]) for i, j in zip(rows, cols))
        return G, set(U), set(V)
    else:
        # Adyacencia A (cuadrada)
        A = np.array(M, dtype=float)
        G = nx.from_numpy_array((A + A.T) / 2.0)  # simetrizar por seguridad
        G.remove_edges_from(nx.selfloop_edges(G))
        if not bx.is_bipartite(G):
            raise ValueError("La matriz cuadrada no define un grafo bipartito.")
        part_U, part_V = bx.sets(G)
        return G, set(part_U), set(part_V)

def bipartite_clustering_stats(G: nx.Graph, U: set, V: set):
    """
    Clustering bipartito (Latapy) por nodo y promedios globales.
    Devuelve dict con:
      - n_nodes, n_edges
      - C_global (promedio en todos los nodos)
      - C_mean_U, C_mean_V (promedios por partición)
    """
    if G.number_of_nodes() == 0:
        return dict(n_nodes=0, n_edges=0, C_global=np.nan, C_mean_U=np.nan, C_mean_V=np.nan)

    C_all = bx.clustering(G)  # Latapy 2008
    vals_all = list(C_all.values())
    C_global = float(np.mean(vals_all)) if vals_all else np.nan
    C_mean_U = float(np.mean([C_all[n] for n in U])) if U else np.nan
    C_mean_V = float(np.mean([C_all[n] for n in V])) if V else np.nan

    return dict(
        n_nodes=G.number_of_nodes(),
        n_edges=G.number_of_edges(),
        C_global=C_global,
        C_mean_U=C_mean_U,
        C_mean_V=C_mean_V
    )

# === RECORRIDO DE ARCHIVOS ===
input_dir = pathlib.Path(INPUT_DIR)
xlsx_files = sorted(input_dir.glob("*.xlsx"))

rows = []

for xfile in xlsx_files:
    layer = layer_from_filename(xfile)
    xl = pd.ExcelFile(xfile)
    for sheet in xl.sheet_names:
        year_sheet = infer_year(sheet)
        year_file = infer_year(xfile.stem)
        year = year_sheet or year_file
        if year not in ANIOS_PERMITIDOS:
            continue
        df = xl.parse(sheet_name=sheet, header=None)
        M = to_numeric_matrix(df)
        try:
            G, U, V = build_bipartite_from_matrix(M)
            stats = bipartite_clustering_stats(G, U, V)
            rows.append({
                "layer": layer,
                "year": year,
                "sheet": sheet,
                "n_nodes": stats["n_nodes"],
                "n_edges": stats["n_edges"],
                "C_global": stats["C_global"],
                "C_mean_U": stats["C_mean_U"],
                "C_mean_V": stats["C_mean_V"],
                "file": xfile.name,
            })
        except Exception as e:
            rows.append({
                "layer": layer,
                "year": year,
                "sheet": sheet,
                "n_nodes": np.nan,
                "n_edges": np.nan,
                "C_global": np.nan,
                "C_mean_U": np.nan,
                "C_mean_V": np.nan,
                "file": xfile.name,
                "error": str(e),
            })

resultados = pd.DataFrame(rows).sort_values(["layer", "year", "sheet"]).reset_index(drop=True)
print(resultados)

# Guardar resultados combinados junto al directorio de entrada
out_path = input_dir / "clustering_bipartito_2012_2017.xlsx"
with pd.ExcelWriter(out_path, engine="openpyxl") as ew:
    resultados.to_excel(ew, index=False, sheet_name="clustering")

print(f"\nArchivo con resultados guardado en: {out_path}")